{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7dciCqe2Atx"
      },
      "outputs": [],
      "source": [
        "#ë¶„ì„ ë² ì´ìŠ¤ ë°ì´í„° ë¡œë“œ\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import ast\n",
        "import json\n",
        "\n",
        "# Long í¬ë§· ë°ì´í„° ë¡œë“œ\n",
        "df_long = pd.read_csv(\"mp_survey/processed/responses_long.csv\")\n",
        "\n",
        "print(\"ğŸ“Š ë¶„ì„ ì‹œì‘!\")\n",
        "print(f\"ğŸ‘¥ ì‚¬ìš©ì ìˆ˜: {df_long['user_id'].nunique()}\")\n",
        "print(f\"ğŸ“ ì´ ì‘ë‹µ ìˆ˜: {len(df_long)}\")\n",
        "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
        "print(df_long.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ëª¨ë“  ì‚¬ìš©ì ë¶„ì„\n",
        "def analyze_user_ego(user_id):\n",
        "    \"\"\"í•œ ì‚¬ìš©ì ìì•„ë³„ ì™„ì „ ë¶„ì„\"\"\"\n",
        "    df_u = df_long[df_long['user_id'] == user_id].copy()\n",
        "\n",
        "    result = {}\n",
        "    for ego in ['ë³¸ëŠ¥ ìì•„', 'ê´€ê³„ ìì•„', 'ëª©í‘œ ìì•„']:\n",
        "        df_e = df_u[df_u['ego'] == ego]\n",
        "\n",
        "        if len(df_e) == 0:\n",
        "            result[ego] = {\"themes\": {}, \"avg_score\": 0, \"percent\": 0}\n",
        "            continue\n",
        "\n",
        "        # ğŸ” í…Œë§ˆ ë¹ˆë„\n",
        "        themes_flat = []\n",
        "        for themes_str in df_e['themes']:\n",
        "            try:\n",
        "                themes = ast.literal_eval(themes_str)\n",
        "                themes_flat.extend(themes)\n",
        "            except:\n",
        "                themes_flat.append(themes_str)\n",
        "\n",
        "        theme_counts = dict(Counter(themes_flat).most_common(10))\n",
        "\n",
        "        # ğŸ“Š í‰ê·  ì ìˆ˜ â†’ íƒìƒ‰ ì •ë„ (%)\n",
        "        avg_score = df_e['score'].mean()\n",
        "        percent = round((avg_score - 1) / 4 * 100, 1)  # 1~5 â†’ 0~100\n",
        "\n",
        "        result[ego] = {\n",
        "            \"theme_counts\": theme_counts,\n",
        "            \"avg_score\": round(avg_score, 2),\n",
        "            \"percent\": percent,\n",
        "            \"top_themes\": [(t, c) for t, c in theme_counts.items()][:5]\n",
        "        }\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"âœ… ì‚¬ìš©ì ë¶„ì„ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ!\")\n"
      ],
      "metadata": {
        "id": "MXdQGK2D2FBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë“  ì‚¬ìš©ì ë¶„ì„ (30ëª…ì´ë¼ë©´ 3ì´ˆ!)\n",
        "users = df_long['user_id'].unique()\n",
        "all_user_analysis = {}\n",
        "\n",
        "for user_id in users:\n",
        "    analysis = analyze_user_ego(user_id)\n",
        "    all_user_analysis[user_id] = analysis\n",
        "\n",
        "    print(f\"âœ… {user_id}: ë¶„ì„ ì™„ë£Œ\")\n",
        "\n",
        "# JSONìœ¼ë¡œ ì €ì¥\n",
        "with open(\"mp_survey/processed/all_user_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_user_analysis, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nğŸ‰ {len(users)}ëª… ì‚¬ìš©ì ë¶„ì„ ì™„ë£Œ!\")\n",
        "print(\"ğŸ“ mp_survey/processed/all_user_analysis.json ì €ì¥ë¨\")\n"
      ],
      "metadata": {
        "id": "hSjnKwtC2HHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ìì•„ë³„ íƒìƒ‰ ì •ë„ ì‹œê°í™”\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ì²« ë²ˆì§¸ ì‚¬ìš©ì ì˜ˆì‹œë¡œ ì‹œê°í™”\n",
        "sample_user = list(all_user_analysis.keys())[0]\n",
        "user_data = all_user_analysis[sample_user]\n",
        "\n",
        "# ë„ë„› ì°¨íŠ¸ (íƒìƒ‰ ì •ë„)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "egos = list(user_data.keys())\n",
        "percents = [user_data[ego]['percent'] for ego in egos]\n",
        "\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "wedges, texts, autotexts = ax.pie(percents, labels=egos, autopct='%1.1f%%',\n",
        "                                  colors=colors, startangle=90, pctdistance=0.85)\n",
        "\n",
        "# ë„ë„› ëª¨ì–‘\n",
        "centre_circle = plt.Circle((0,0), 0.70, fc='white')\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "plt.title(f\"{sample_user} - ìì•„ë³„ íƒìƒ‰ ì •ë„\", fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“Š ì²« ë²ˆì§¸ ì‚¬ìš©ì ìì•„ íƒìƒ‰ ì •ë„:\")\n",
        "for ego, data in user_data.items():\n",
        "    print(f\"  {ego}: {data['percent']}% (í‰ê·  {data['avg_score']:.2f}ì )\")\n"
      ],
      "metadata": {
        "id": "14nBpLTN2IMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ìì•„ë³„ í‚¤ì›Œë“œ ìˆœìœ„ + ì›Œë“œ í´ë¼ìš°ë“œ\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def create_user_wordclouds(user_id):\n",
        "    \"\"\"ì‚¬ìš©ìë³„ ìì•„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\"\"\"\n",
        "    analysis = all_user_analysis[user_id]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    for i, ego in enumerate(['ë³¸ëŠ¥ ìì•„', 'ê´€ê³„ ìì•„', 'ëª©í‘œ ìì•„']):\n",
        "        counts = analysis[ego]['theme_counts']\n",
        "        if not counts:\n",
        "            axes[i].text(0.5, 0.5, 'ë°ì´í„° ì—†ìŒ', ha='center', va='center', transform=axes[i].transAxes)\n",
        "            axes[i].set_title(ego)\n",
        "            continue\n",
        "\n",
        "        # ì›Œë“œí´ë¼ìš°ë“œ\n",
        "        wc = WordCloud(width=400, height=400, background_color='white',\n",
        "                      colormap='viridis', font_path=None).generate_from_frequencies(counts)\n",
        "        axes[i].imshow(wc, interpolation='bilinear')\n",
        "        axes[i].set_title(f\"{ego}\\n({analysis[ego]['percent']}%)\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"mp_survey/users/{user_id}_wordclouds.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return f\"mp_survey/users/{user_id}_wordclouds.png\"\n",
        "\n",
        "# ì²« 3ëª… ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
        "for user_id in list(all_user_analysis.keys())[:3]:\n",
        "    print(f\"\\nğŸ¨ {user_id} ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±:\")\n",
        "    img_path = create_user_wordclouds(user_id)\n"
      ],
      "metadata": {
        "id": "8vd9Y-LB2Onq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM ìš”ì•½ + ê°€ì´ë“œ ìƒì„± ì¤€ë¹„ (open AI í™œìš©)\n",
        "# OpenAI API í‚¤ ì…ë ¥ (ë¹„ë°€)\n",
        "import openai\n",
        "# openai.api_key = \"your-api-key-here\"\n",
        "\n",
        "def llm_summarize_user(user_id):\n",
        "    \"\"\"LLMìœ¼ë¡œ ìì•„ ìš”ì•½ + ê°€ì´ë“œ ìƒì„±\"\"\"\n",
        "    analysis = all_user_analysis[user_id]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "ì‚¬ìš©ì {user_id}ì˜ ìì•„ ë¶„ì„ ê²°ê³¼:\n",
        "\n",
        "\"\"\"\n",
        "    for ego, data in analysis.items():\n",
        "        top5 = ', '.join([t for t,c in data['top_themes']])\n",
        "        prompt += f\"- {ego}: {data['percent']}%, ì£¼ìš” í‚¤ì›Œë“œ: [{top5}]\\n\"\n",
        "\n",
        "    prompt += \"\"\"\n",
        "ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ:\n",
        "1. ê° ìì•„ 1ì¤„ì”© ìš”ì•½ (ì´ 3ì¤„)\n",
        "2. íƒìƒ‰ ì •ë„ ë‚®ì€ ìì•„ ì¤‘ì‹¬ ê°€ì´ë“œ (2ê°€ì§€ ì—°ìŠµ)\n",
        "3. ì„¸ ìì•„ ê· í˜• ê°€ì´ë“œ (2~3ì¤„)\n",
        "\n",
        "ë”°ëœ»í•˜ê³  ì¤‘ë¦½ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.\n",
        "\"\"\"\n",
        "\n",
        "    # response = openai.ChatCompletion.create(\n",
        "    #     model=\"gpt-3.5-turbo\",\n",
        "    #     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    # )\n",
        "\n",
        "    # LLM ì‘ë‹µ íŒŒì‹±í•´ì„œ JSON ì €ì¥ (êµ¬í˜„ì‹œ)\n",
        "    llm_result = {\n",
        "        \"summary\": {ego: f\"{ego} ìš”ì•½ ì˜ˆì •\" for ego in analysis.keys()},\n",
        "        \"exploration_guide\": \"íƒìƒ‰ ê°€ì´ë“œ ì˜ˆì •\",\n",
        "        \"balance_guide\": \"ê· í˜• ê°€ì´ë“œ ì˜ˆì •\"\n",
        "    }\n",
        "\n",
        "    # ì‚¬ìš©ìë³„ LLM ê²°ê³¼ ì €ì¥\n",
        "    with open(f\"mp_survey/users/{user_id}_llm_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(llm_result, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return llm_result\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ (ì²« ì‚¬ìš©ì)\n",
        "print(\"ğŸ¤– LLM ìš”ì•½ í…ŒìŠ¤íŠ¸:\")\n",
        "llm_result = llm_summarize_user(list(all_user_analysis.keys())[0])\n",
        "print(json.dumps(llm_result, ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "id": "5Dpzx0JX2TDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ‰ ëª¨ë“  ì‚¬ìš©ì ê²°ê³¼ ìë™ ìƒì„±!\n",
        "results_summary = []\n",
        "\n",
        "for user_id in list(all_user_analysis.keys()):\n",
        "    analysis = all_user_analysis[user_id]\n",
        "\n",
        "    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
        "    create_user_wordclouds(user_id)\n",
        "\n",
        "    # LLM ìš”ì•½ (í…ŒìŠ¤íŠ¸ìš©)\n",
        "    llm_result = llm_summarize_user(user_id)\n",
        "\n",
        "    # ìµœì¢… ì‚¬ìš©ì ë¦¬í¬íŠ¸ JSON\n",
        "    user_report = {\n",
        "        \"user_id\": user_id,\n",
        "        \"ego_overview\": analysis,\n",
        "        \"wordcloud_path\": f\"mp_survey/users/{user_id}_wordclouds.png\",\n",
        "        \"llm_summary\": llm_result\n",
        "    }\n",
        "\n",
        "    # ì‚¬ìš©ìë³„ JSON ì €ì¥\n",
        "    with open(f\"mp_survey/users/{user_id}_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(user_report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    results_summary.append({\n",
        "        \"user_id\": user_id,\n",
        "        \"ë³¸ëŠ¥\": analysis['ë³¸ëŠ¥ ìì•„']['percent'],\n",
        "        \"ê´€ê³„\": analysis['ê´€ê³„ ìì•„']['percent'],\n",
        "        \"ëª©í‘œ\": analysis['ëª©í‘œ ìì•„']['percent']\n",
        "    })\n",
        "\n",
        "    print(f\"âœ… {user_id} ì™„ë£Œ\")\n",
        "\n",
        "# ì „ì²´ ìš”ì•½ CSV\n",
        "pd.DataFrame(results_summary).to_csv(\"mp_survey/processed/user_summary.csv\", index=False)\n",
        "print(\"\\nğŸŠ ì „ì²´ ë¶„ì„ ì™„ë£Œ!\")\n",
        "print(\"ğŸ“ mp_survey/users/ í´ë”ì— ì‚¬ìš©ìë³„ ê²°ê³¼ ì €ì¥ë¨\")\n"
      ],
      "metadata": {
        "id": "8AHcVNBp2YUt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}